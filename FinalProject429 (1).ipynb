{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 1"
      ],
      "metadata": {
        "id": "5bsDV8EqSx7O"
      },
      "id": "5bsDV8EqSx7O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdc7cf5-9286-4530-9b35-060413ce7da0",
      "metadata": {
        "id": "3cdc7cf5-9286-4530-9b35-060413ce7da0"
      },
      "outputs": [],
      "source": [
        "# If you're in Colab, you may need to install these once:\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install scikit-learn\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub\n",
        "import warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea7a5e4-330b-41b5-8969-eb22e28d8cac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ea7a5e4-330b-41b5-8969-eb22e28d8cac",
        "outputId": "f098dbb6-836e-431c-eede-9e499bc5d4c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "SEED = 429\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tVoZqqtiRnco",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVoZqqtiRnco",
        "outputId": "91562c4a-55fe-48a5-c647-a65e9b896401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'asl-alphabet' dataset.\n",
            "Path to dataset files: /kaggle/input/asl-alphabet\n"
          ]
        }
      ],
      "source": [
        "path=kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "print(\"Path to dataset files:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8a117d-2482-456f-9733-8ccb837d9fa6",
      "metadata": {
        "id": "1f8a117d-2482-456f-9733-8ccb837d9fa6"
      },
      "outputs": [],
      "source": [
        "data_root=os.path.join(path, \"asl_alphabet_train\", \"asl_alphabet_train\")\n",
        "TRAIN_DIR=Path(data_root)\n",
        "input_size = 224  # ResNet default\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225]),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e578ce9b-8a40-47cb-bf4e-fcd6f065220a",
      "metadata": {
        "id": "e578ce9b-8a40-47cb-bf4e-fcd6f065220a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f81c28-c852-49ec-f362-ae37e9fbe60b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "base_train_dataset = datasets.ImageFolder(root=data_root, transform=train_transform)\n",
        "num_classes = len(base_train_dataset.classes)\n",
        "num_classes, base_train_dataset.classes[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e07d07-7c9c-477b-b730-8cb669ae5c4f",
      "metadata": {
        "id": "f1e07d07-7c9c-477b-b730-8cb669ae5c4f"
      },
      "outputs": [],
      "source": [
        "class ASLSplitDataset(Dataset):\n",
        "    def __init__(self, base_dataset, indices, transform=None):\n",
        "        self.base = base_dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "        self.samples = [self.base.samples[i] for i in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = self.base.loader(img_path)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5840576d-0279-48f7-b733-73c454bbe6a8",
      "metadata": {
        "id": "5840576d-0279-48f7-b733-73c454bbe6a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa95c93e-f150-4be3-cfcd-13ce974b4a60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69600, 17400)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "indices = np.arange(len(base_train_dataset))\n",
        "labels = np.array([s[1] for s in base_train_dataset.samples])\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.2,\n",
        "    stratify=labels,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "len(train_idx), len(val_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a33273-6744-4631-8186-869b17d3a9fb",
      "metadata": {
        "id": "48a33273-6744-4631-8186-869b17d3a9fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ea2ee3-7e8c-4d2e-dc1f-ff987296edb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataset = ASLSplitDataset(base_train_dataset, train_idx, transform=train_transform)\n",
        "val_dataset   = ASLSplitDataset(base_train_dataset, val_idx,   transform=eval_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9bea829-070a-4977-8590-e8cb94434ad2",
      "metadata": {
        "id": "b9bea829-070a-4977-8590-e8cb94434ad2"
      },
      "outputs": [],
      "source": [
        "def build_resnet18(num_classes, pretrained=True):\n",
        "    \"\"\"\n",
        "    Returns a ResNet-18 with the classifier head replaced to match num_classes.\n",
        "    Uses ImageNet weights when pretrained=True.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        model = models.resnet18(weights=weights)\n",
        "    except AttributeError:\n",
        "        # For older torchvision\n",
        "        model = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b0a0bc-66ff-4e95-b3ac-4086e601afcc",
      "metadata": {
        "id": "92b0a0bc-66ff-4e95-b3ac-4086e601afcc"
      },
      "outputs": [],
      "source": [
        "def set_trainable_layers(model, mode):\n",
        "    \"\"\"\n",
        "    mode:\n",
        "      'T-A': head-only\n",
        "      'T-B': last block (layer4 + fc)\n",
        "      'T-C': progressive: layer3 + layer4 + fc\n",
        "      'S-A': from scratch (all trainable)\n",
        "    \"\"\"\n",
        "    # start with all trainable\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    if mode == \"T-A\":\n",
        "        # Freeze entire backbone, train only fc\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in model.fc.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    elif mode == \"T-B\":\n",
        "        for name, p in model.named_parameters():\n",
        "            if (name.startswith(\"conv1\") or name.startswith(\"bn1\") or\n",
        "                name.startswith(\"layer1\") or name.startswith(\"layer2\") or\n",
        "                name.startswith(\"layer3\")):\n",
        "                p.requires_grad = False\n",
        "            else:\n",
        "                p.requires_grad = True\n",
        "\n",
        "    elif mode == \"T-C\":\n",
        "        # Freeze stem + layer1 + layer2, train layer3, layer4, fc\n",
        "        for name, p in model.named_parameters():\n",
        "            if (name.startswith(\"conv1\") or name.startswith(\"bn1\") or\n",
        "                name.startswith(\"layer1\") or name.startswith(\"layer2\")):\n",
        "                p.requires_grad = False\n",
        "            else:\n",
        "                p.requires_grad = True\n",
        "\n",
        "    elif mode == \"S-A\":\n",
        "        # all trainable, but model should be built with pretrained=False for S-A\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989b58e8-20ea-417e-bc90-33d732055ede",
      "metadata": {
        "id": "989b58e8-20ea-417e-bc90-33d732055ede"
      },
      "outputs": [],
      "source": [
        "def set_frozen_bn_eval(model):\n",
        "    \"\"\"\n",
        "    For any BatchNorm layer whose parameters are all frozen (requires_grad=False),\n",
        "    set the module to eval() so running stats don't update.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            all_frozen = all(not p.requires_grad for p in m.parameters())\n",
        "            if all_frozen:\n",
        "                m.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7407a1ca-46bb-4bf7-86fe-65a424bd7bfc",
      "metadata": {
        "id": "7407a1ca-46bb-4bf7-86fe-65a424bd7bfc"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    set_frozen_bn_eval(model)  # important for frozen configs\n",
        "\n",
        "    running_loss = 0.0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1, np.array(y_true), np.array(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5f0484-e0ab-488a-9016-c48c2c37e3a3",
      "metadata": {
        "id": "9f5f0484-e0ab-488a-9016-c48c2c37e3a3"
      },
      "outputs": [],
      "source": [
        "def train_model_experiment(mode, num_epochs=1, base_lr=1e-3, pretrained=True,\n",
        "                           train_loader=train_loader, val_loader=val_loader,\n",
        "                           save_dir=\"checkpoints\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    if mode == \"S-A\":\n",
        "        model = build_resnet18(num_classes, pretrained=False)\n",
        "    else:\n",
        "        model = build_resnet18(num_classes, pretrained=pretrained)\n",
        "\n",
        "    model = set_trainable_layers(model, mode)\n",
        "    model.to(device)\n",
        "\n",
        "    # Only optimize trainable params\n",
        "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(params_to_update, lr=base_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"train_acc\": [], \"train_f1\": [],\n",
        "        \"val_loss\": [],   \"val_acc\": [],   \"val_f1\": [],\n",
        "    }\n",
        "\n",
        "    best_val_f1 = -np.inf\n",
        "    best_state_dict = None\n",
        "    best_epoch = -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc, val_f1, y_true, y_pred = eval_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"train_f1\"].append(train_f1)\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "        print(f\"[{mode}] Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train loss: {train_loss:.4f} acc: {train_acc:.4f} f1: {train_f1:.4f} | \"\n",
        "              f\"Val loss: {val_loss:.4f} acc: {val_acc:.4f} f1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_epoch = epoch\n",
        "            best_state_dict = model.state_dict().copy()\n",
        "            ckpt_path = os.path.join(save_dir, f\"resnet18_{mode}_best.pt\")\n",
        "            torch.save(best_state_dict, ckpt_path)\n",
        "\n",
        "    print(f\"\\nBest {mode} val macro-F1: {best_val_f1:.4f} (epoch {best_epoch+1})\")\n",
        "    return history, best_state_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ff92c3-b530-40e4-bf30-715f82d71d8f",
      "metadata": {
        "id": "46ff92c3-b530-40e4-bf30-715f82d71d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "286eaaf2-6a88-413d-8681-a679653445ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 176MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1991248090.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history_TA, state_TA = train_model_experiment(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"T-A\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbase_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2869717877.py\u001b[0m in \u001b[0;36mtrain_model_experiment\u001b[0;34m(mode, num_epochs, base_lr, pretrained, train_loader, val_loader, save_dir)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-158432795.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history_TA, state_TA = train_model_experiment(\n",
        "    mode=\"T-A\",\n",
        "    num_epochs=3,\n",
        "    base_lr=1e-3,\n",
        "    pretrained=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30775724-0054-48b3-b365-8ca9588cbcd2",
      "metadata": {
        "id": "30775724-0054-48b3-b365-8ca9588cbcd2"
      },
      "outputs": [],
      "source": [
        "history_TB, state_TB = train_model_experiment(\n",
        "    mode=\"T-B\",\n",
        "    num_epochs=3,\n",
        "    base_lr=5e-4,\n",
        "    pretrained=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acedb03a-b189-4ce2-a651-9bce6db621b7",
      "metadata": {
        "id": "acedb03a-b189-4ce2-a651-9bce6db621b7"
      },
      "outputs": [],
      "source": [
        "# Build model with pretrained backbone, load T-B best weights, then unfreeze layer3\n",
        "model_TC = build_resnet18(num_classes, pretrained=True)\n",
        "ckpt_TB_path = \"checkpoints/resnet18_T-B_best.pt\"\n",
        "model_TC.load_state_dict(torch.load(ckpt_TB_path, map_location=device))\n",
        "\n",
        "model_TC = set_trainable_layers(model_TC, \"T-C\")\n",
        "model_TC.to(device)\n",
        "\n",
        "params_to_update = [p for p in model_TC.parameters() if p.requires_grad]\n",
        "optimizer_TC = optim.Adam(params_to_update, lr=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "history_TC = {\n",
        "    \"train_loss\": [], \"train_acc\": [], \"train_f1\": [],\n",
        "    \"val_loss\": [],   \"val_acc\": [],   \"val_f1\": [],\n",
        "}\n",
        "\n",
        "best_val_f1 = -np.inf\n",
        "best_state_TC = None\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(3):\n",
        "    train_loss, train_acc, train_f1 = train_one_epoch(model_TC, train_loader, optimizer_TC, criterion, device)\n",
        "    val_loss, val_acc, val_f1, y_true, y_pred = eval_one_epoch(model_TC, val_loader, criterion, device)\n",
        "\n",
        "    history_TC[\"train_loss\"].append(train_loss)\n",
        "    history_TC[\"train_acc\"].append(train_acc)\n",
        "    history_TC[\"train_f1\"].append(train_f1)\n",
        "    history_TC[\"val_loss\"].append(val_loss)\n",
        "    history_TC[\"val_acc\"].append(val_acc)\n",
        "    history_TC[\"val_f1\"].append(val_f1)\n",
        "\n",
        "    print(f\"[T-C] Epoch {epoch+1}/10 | \"\n",
        "          f\"Train loss: {train_loss:.4f} acc: {train_acc:.4f} f1: {train_f1:.4f} | \"\n",
        "          f\"Val loss: {val_loss:.4f} acc: {val_acc:.4f} f1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_state_TC = model_TC.state_dict().copy()\n",
        "        best_epoch = epoch\n",
        "        torch.save(best_state_TC, \"checkpoints/resnet18_T-C_best.pt\")\n",
        "\n",
        "print(f\"\\nBest T-C val macro-F1: {best_val_f1:.4f} (epoch {best_epoch+1})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d957f9b-41c4-4969-b028-4bf1485bb459",
      "metadata": {
        "id": "9d957f9b-41c4-4969-b028-4bf1485bb459"
      },
      "outputs": [],
      "source": [
        "history_SA, state_SA = train_model_experiment(\n",
        "    mode=\"S-A\",\n",
        "    num_epochs=3,      # you may want more epochs from scratch\n",
        "    base_lr=1e-3,\n",
        "    pretrained=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e713b7-c0f3-40f8-b708-209b1b9b5af8",
      "metadata": {
        "id": "c5e713b7-c0f3-40f8-b708-209b1b9b5af8"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, title_prefix=\"\"):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    axes[0].plot(epochs, history[\"train_loss\"], label=\"Train loss\", marker='o')\n",
        "    axes[0].plot(epochs, history[\"val_loss\"], label=\"Val loss\", marker='o')\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].set_title(f\"{title_prefix} Loss\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    axes[1].plot(epochs, history[\"train_acc\"], label=\"Train acc\", marker='o')\n",
        "    axes[1].plot(epochs, history[\"val_acc\"], label=\"Val acc\", marker='o')\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Accuracy\")\n",
        "    axes[1].set_title(f\"{title_prefix} Accuracy\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Plot Macro-F1\n",
        "    axes[2].plot(epochs, history[\"train_f1\"], label=\"Train macro-F1\", marker='o')\n",
        "    axes[2].plot(epochs, history[\"val_f1\"], label=\"Val macro-F1\", marker='o')\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].set_ylabel(\"Macro-F1\")\n",
        "    axes[2].set_title(f\"{title_prefix} Macro-F1\")\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6833209-ac14-48be-a9cd-a7949c6f2169",
      "metadata": {
        "id": "d6833209-ac14-48be-a9cd-a7949c6f2169"
      },
      "outputs": [],
      "source": [
        "plot_history(history_TA, \"T-A\")\n",
        "plot_history(history_TB, \"T-B\")\n",
        "plot_history(history_TC, \"T-C\")\n",
        "plot_history(history_SA, \"S-A\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c71354f-46b5-47fa-8984-0dadaf81ef8f",
      "metadata": {
        "id": "4c71354f-46b5-47fa-8984-0dadaf81ef8f"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model_on_loader(ckpt_path, mode_name, loader):\n",
        "    model = build_resnet18(num_classes, pretrained=False)\n",
        "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss, acc, f1, y_true, y_pred = eval_one_epoch(model, loader, criterion, device)\n",
        "\n",
        "    print(f\"[{mode_name}] Loss: {loss:.4f}, Acc: {acc:.4f}, Macro-F1: {f1:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred, target_names=base_train_dataset.classes, digits=4\n",
        "    ))\n",
        "\n",
        "    return loss, acc, f1, cm, y_true, y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04aea0b7-e688-4741-bf93-f80da8c4ac2e",
      "metadata": {
        "id": "04aea0b7-e688-4741-bf93-f80da8c4ac2e"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "class UnsortedImages(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = Path(root)\n",
        "        self.paths = [p for p in self.root.iterdir() if p.is_file()]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, img_path.name     # return filename instead of label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MY_TEST_DIR = \"/content/drive/MyDrive/my_asl_test\"\n",
        "\n",
        "my_test_dataset = UnsortedImages(MY_TEST_DIR, transform=eval_transform)\n",
        "my_test_loader  = DataLoader(my_test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "Dh4Ue7AdLYIL"
      },
      "id": "Dh4Ue7AdLYIL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BEST_MODE = \"T-C\"  # or whatever actually won\n",
        "BEST_CKPT = f\"checkpoints/resnet18_{BEST_MODE}_best.pt\"\n",
        "\n",
        "model = build_resnet18(num_classes, pretrained=False)\n",
        "model.load_state_dict(torch.load(BEST_CKPT, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, names in my_test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "        all_preds.extend(list(zip(names, preds)))"
      ],
      "metadata": {
        "id": "moMZSu9qLbc6"
      },
      "id": "moMZSu9qLbc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_phase1_confusion(ckpt_path, loader, mode_name=\"Phase1\"):\n",
        "    # Build the same architecture and load the checkpoint\n",
        "    model = build_resnet18(num_classes, pretrained=False)\n",
        "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for imgs, labels in loader:          # <-- loader must yield (images, labels)\n",
        "        imgs  = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"[{mode_name}] confusion matrix:\\n\", cm)\n",
        "\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred, target_names=base_train_dataset.classes, digits=4\n",
        "    ))\n",
        "\n",
        "    return cm, y_true, y_pred\n"
      ],
      "metadata": {
        "id": "wgh7jCLgTV8i"
      },
      "id": "wgh7jCLgTV8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_val, y_true_val, y_pred_val = evaluate_phase1_confusion(\n",
        "    BEST_CKPT, val_loader, mode_name=f\"{BEST_MODE} (ASL val)\"\n",
        ")"
      ],
      "metadata": {
        "id": "0jWnFsuKTYpR"
      },
      "id": "0jWnFsuKTYpR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, pred in all_preds:\n",
        "    print(f\"{name} --> {base_train_dataset.classes[pred]}\")\n"
      ],
      "metadata": {
        "id": "xWGQVNLiMYwl"
      },
      "id": "xWGQVNLiMYwl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 2: BONUS\n"
      ],
      "metadata": {
        "id": "flbr9DGqWn4n"
      },
      "id": "flbr9DGqWn4n"
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    \"T-A\": max(history_TA[\"val_f1\"]),\n",
        "    \"T-B\": max(history_TB[\"val_f1\"]),\n",
        "    \"T-C\": max(history_TC[\"val_f1\"]),\n",
        "    \"S-A\": max(history_SA[\"val_f1\"]),\n",
        "}\n",
        "print(results)\n",
        "\n",
        "best_mode = max(results, key=results.get)\n",
        "print(\"Best mode:\", best_mode, \"with val macro-F1 =\", results[best_mode])\n"
      ],
      "metadata": {
        "id": "ns3ZxMzrXw_S"
      },
      "id": "ns3ZxMzrXw_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResNetFrameEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Takes a Phase-1 ResNet-18 checkpoint and returns per-frame feature vectors.\n",
        "    Controls freezing policy for Stage 2A / 2B.\n",
        "    \"\"\"\n",
        "    def __init__(self, ckpt_path, num_classes, freeze_all=True, unfreeze_layer4=False):\n",
        "        super().__init__()\n",
        "        # Build same architecture as Phase 1 and load weights\n",
        "        backbone = build_resnet18(num_classes, pretrained=False)\n",
        "        state = torch.load(ckpt_path, map_location=device)\n",
        "        backbone.load_state_dict(state)\n",
        "\n",
        "        # Save blocks explicitly so we can control layer4\n",
        "        self.conv1 = backbone.conv1\n",
        "        self.bn1   = backbone.bn1\n",
        "        self.relu  = backbone.relu\n",
        "        self.maxpool = backbone.maxpool\n",
        "        self.layer1 = backbone.layer1\n",
        "        self.layer2 = backbone.layer2\n",
        "        self.layer3 = backbone.layer3\n",
        "        self.layer4 = backbone.layer4\n",
        "        self.avgpool = backbone.avgpool  # global average pooling\n",
        "        self.feature_dim = 512  # ResNet-18\n",
        "\n",
        "        # Freeze all by default\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = not freeze_all\n",
        "\n",
        "        # For Stage 2B: unfreeze only layer4\n",
        "        if unfreeze_layer4:\n",
        "            for p in self.layer4.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):   # x: (B, C, H, W)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)           # (B, 512, 1, 1)\n",
        "        x = torch.flatten(x, 1)       # (B, 512)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cJ0JzVqEYDYU"
      },
      "id": "cJ0JzVqEYDYU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ASLTemporalModel(nn.Module):\n",
        "    def __init__(self, frame_encoder, feature_dim=512,\n",
        "                 hidden_dim=256, num_layers=1, num_classes=100):\n",
        "        \"\"\"\n",
        "        frame_encoder: ResNetFrameEncoder\n",
        "        num_classes: number of WLASL word classes (100 for WLASL100 subset)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.frame_encoder = frame_encoder\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=feature_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, frames):  # frames: (B, T, C, H, W)\n",
        "        B, T, C, H, W = frames.shape\n",
        "        frames = frames.view(B * T, C, H, W)\n",
        "\n",
        "        # Encode each frame\n",
        "        feats = self.frame_encoder(frames)     # (B*T, 512)\n",
        "        feats = feats.view(B, T, -1)          # (B, T, 512)\n",
        "\n",
        "        # LSTM over time\n",
        "        outputs, _ = self.lstm(feats)         # (B, T, 2*hidden_dim)\n",
        "        last_output = outputs[:, -1, :]       # (B, 2*hidden_dim)\n",
        "\n",
        "        logits = self.classifier(last_output) # (B, num_classes)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "0jsgZjbjYDWH"
      },
      "id": "0jsgZjbjYDWH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class WLASLFramesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expects structure:\n",
        "      root/\n",
        "        class_0/\n",
        "          vid_0001/\n",
        "             frame_0001.jpg\n",
        "             frame_0002.jpg\n",
        "             ...\n",
        "          vid_0002/\n",
        "             ...\n",
        "        class_1/\n",
        "          vid_0003/\n",
        "             ...\n",
        "    Each 'vid_xxx' directory is treated as one sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform=None, max_frames=16):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "\n",
        "        # class folders under root_dir\n",
        "        class_dirs = [d for d in self.root_dir.iterdir() if d.is_dir()]\n",
        "        self.class_names = sorted(d.name for d in class_dirs)\n",
        "        self.class_to_idx = {name: i for i, name in enumerate(self.class_names)}\n",
        "\n",
        "        self.samples = []  # list of (list_of_frame_paths, class_idx)\n",
        "\n",
        "        for class_dir in class_dirs:\n",
        "            cls_idx = self.class_to_idx[class_dir.name]\n",
        "            # each subfolder is a video instance\n",
        "            for vid_dir in class_dir.iterdir():\n",
        "                if not vid_dir.is_dir():\n",
        "                    continue\n",
        "                frame_files = sorted(\n",
        "                    list(vid_dir.glob(\"*.jpg\")) +\n",
        "                    list(vid_dir.glob(\"*.jpeg\")) +\n",
        "                    list(vid_dir.glob(\"*.png\"))\n",
        "                )\n",
        "                if len(frame_files) == 0:\n",
        "                    continue\n",
        "                self.samples.append((frame_files, cls_idx))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} video sequences from {self.root_dir}\")\n",
        "        print(f\"Num classes: {len(self.class_names)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _sample_or_pad_indices(self, num_frames):\n",
        "        if num_frames >= self.max_frames:\n",
        "            idxs = np.linspace(0, num_frames - 1, self.max_frames).astype(int)\n",
        "        else:\n",
        "            base = np.arange(num_frames)\n",
        "            reps = int(np.ceil(self.max_frames / num_frames))\n",
        "            idxs = np.tile(base, reps)[:self.max_frames]\n",
        "        return idxs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_paths, label = self.samples[idx]\n",
        "        num_frames = len(frame_paths)\n",
        "        idxs = self._sample_or_pad_indices(num_frames)\n",
        "\n",
        "        frames = []\n",
        "        for i in idxs:\n",
        "            img = read_image(str(frame_paths[i]))  # (C, H, W), uint8\n",
        "            img = transforms.functional.convert_image_dtype(img, torch.float32)  # [0,1]\n",
        "            if self.transform:\n",
        "                # convert to PIL for your eval_transform\n",
        "                pil = transforms.ToPILImage()(img)\n",
        "                img = self.transform(pil)\n",
        "            frames.append(img)\n",
        "\n",
        "        frames_tensor = torch.stack(frames, dim=0)  # (T, C, H, W)\n",
        "        return frames_tensor, label\n"
      ],
      "metadata": {
        "id": "97P9BtPOYDUD"
      },
      "id": "97P9BtPOYDUD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub, os, pandas as pd\n",
        "\n",
        "# Download latest version of the WLASL100 dataset\n",
        "path = kagglehub.dataset_download(\"thtrnphc/wlasl100\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# See what's inside that folder so we know what to point to\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "id": "oHZtanslZaur"
      },
      "id": "oHZtanslZaur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_path = os.path.join(path, \"preprocessing\")\n",
        "print(\"preprocessing contains:\", os.listdir(pre_path))\n"
      ],
      "metadata": {
        "id": "Z1WFTaJ_aJK8"
      },
      "id": "Z1WFTaJ_aJK8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "path = '/kaggle/input/wlasl100'\n",
        "\n",
        "pre_path = os.path.join(path, \"preprocessing\")\n",
        "\n",
        "WLASL_TRAIN_FRAMES_ROOT = os.path.join(pre_path, \"train\", \"frames\")\n",
        "WLASL_VAL_FRAMES_ROOT   = os.path.join(pre_path, \"val\",   \"frames\")\n",
        "WLASL_TEST_FRAMES_ROOT  = os.path.join(pre_path, \"test\",  \"frames\")\n",
        "\n",
        "print(\"Train frames root:\", WLASL_TRAIN_FRAMES_ROOT)\n",
        "print(\"Sample class dirs:\", os.listdir(WLASL_TRAIN_FRAMES_ROOT)[:5])"
      ],
      "metadata": {
        "id": "2QPI_n79albD"
      },
      "id": "2QPI_n79albD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wlasl_train_dataset = WLASLFramesDataset(\n",
        "    WLASL_TRAIN_FRAMES_ROOT, transform=eval_transform, max_frames=16\n",
        ")\n",
        "wlasl_val_dataset = WLASLFramesDataset(\n",
        "    WLASL_VAL_FRAMES_ROOT, transform=eval_transform, max_frames=16\n",
        ")\n",
        "wlasl_test_dataset = WLASLFramesDataset(\n",
        "    WLASL_TEST_FRAMES_ROOT, transform=eval_transform, max_frames=16\n",
        ")\n",
        "\n",
        "num_wlasl_classes = len(wlasl_train_dataset.class_names)\n",
        "print(\"Num WLASL classes:\", num_wlasl_classes)\n",
        "\n",
        "wlasl_train_loader = DataLoader(\n",
        "    wlasl_train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True\n",
        ")\n",
        "wlasl_val_loader = DataLoader(\n",
        "    wlasl_val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "wlasl_test_loader = DataLoader(\n",
        "    wlasl_test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Pu7waOGKcApz"
      },
      "id": "Pu7waOGKcApz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames, labels = next(iter(wlasl_train_loader))\n",
        "print(frames.shape)   # should be (B, T, C, H, W)\n",
        "print(labels.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "2D5563_CbYr_"
      },
      "id": "2D5563_CbYr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BEST_MODE = \"T-B\"  # or whatever actually won\n",
        "BEST_CKPT = f\"checkpoints/resnet18_{BEST_MODE}_best.pt\"\n",
        "\n"
      ],
      "metadata": {
        "id": "mwdAMUBlYDQM"
      },
      "id": "mwdAMUBlYDQM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_temporal(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    set_frozen_bn_eval(model)  # still useful for frozen parts of ResNet\n",
        "\n",
        "    running_loss = 0.0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for frames, labels in loader:\n",
        "        frames = frames.to(device)   # (B, T, C, H, W)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch_temporal(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for frames, labels in loader:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    return epoch_loss, epoch_acc, epoch_f1, np.array(y_true), np.array(y_pred)\n"
      ],
      "metadata": {
        "id": "i5acNMDLYDOB"
      },
      "id": "i5acNMDLYDOB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_stage_2A(num_epochs=10, base_lr=1e-3, num_wlasl_classes=100):\n",
        "    # Frame encoder: freeze all CNN parameters\n",
        "    frame_encoder_2A = ResNetFrameEncoder(\n",
        "        ckpt_path=BEST_CKPT,\n",
        "        num_classes=num_classes,      # from Phase 1 ASL, e.g., 29\n",
        "        freeze_all=True,\n",
        "        unfreeze_layer4=False\n",
        "    )\n",
        "\n",
        "    model_2A = ASLTemporalModel(\n",
        "        frame_encoder=frame_encoder_2A,\n",
        "        feature_dim=512,\n",
        "        hidden_dim=256,\n",
        "        num_layers=1,\n",
        "        num_classes=num_wlasl_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Only parameters that require grad are the LSTM + classifier\n",
        "    params_to_update = [p for p in model_2A.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(params_to_update, lr=base_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history_2A = {\"train_loss\": [], \"train_acc\": [], \"train_f1\": [],\n",
        "                  \"val_loss\": [],   \"val_acc\": [],   \"val_f1\": []}\n",
        "\n",
        "    best_val_f1 = -np.inf\n",
        "    best_state_2A = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch_temporal(\n",
        "            model_2A, wlasl_train_loader, optimizer, criterion, device\n",
        "        )\n",
        "        val_loss, val_acc, val_f1, _, _ = eval_one_epoch_temporal(\n",
        "            model_2A, wlasl_val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        history_2A[\"train_loss\"].append(train_loss)\n",
        "        history_2A[\"train_acc\"].append(train_acc)\n",
        "        history_2A[\"train_f1\"].append(train_f1)\n",
        "        history_2A[\"val_loss\"].append(val_loss)\n",
        "        history_2A[\"val_acc\"].append(val_acc)\n",
        "        history_2A[\"val_f1\"].append(val_f1)\n",
        "\n",
        "        print(f\"[2A] Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train loss: {train_loss:.4f}, acc: {train_acc:.4f}, f1: {train_f1:.4f} | \"\n",
        "              f\"Val loss: {val_loss:.4f}, acc: {val_acc:.4f}, f1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state_2A = model_2A.state_dict().copy()\n",
        "            torch.save(best_state_2A, \"checkpoints/temporal_stage2A_best.pt\")\n",
        "\n",
        "    print(f\"\\nBest Stage 2A val macro-F1: {best_val_f1:.4f}\")\n",
        "    return history_2A, best_state_2A\n"
      ],
      "metadata": {
        "id": "y-W7nmJNYDMN"
      },
      "id": "y-W7nmJNYDMN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_stage_2B(num_epochs=10, base_lr=5e-4, num_wlasl_classes=100):\n",
        "    frame_encoder_2B = ResNetFrameEncoder(\n",
        "        ckpt_path=BEST_CKPT,\n",
        "        num_classes=num_classes,\n",
        "        freeze_all=True,\n",
        "        unfreeze_layer4=True   # <--- only layer4 trainable in the CNN\n",
        "    )\n",
        "\n",
        "    model_2B = ASLTemporalModel(\n",
        "        frame_encoder=frame_encoder_2B,\n",
        "        feature_dim=512,\n",
        "        hidden_dim=256,\n",
        "        num_layers=1,\n",
        "        num_classes=num_wlasl_classes\n",
        "    ).to(device)\n",
        "\n",
        "    params_to_update = [p for p in model_2B.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(params_to_update, lr=base_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history_2B = {\"train_loss\": [], \"train_acc\": [], \"train_f1\": [],\n",
        "                  \"val_loss\": [],   \"val_acc\": [],   \"val_f1\": []}\n",
        "\n",
        "    best_val_f1 = -np.inf\n",
        "    best_state_2B = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch_temporal(\n",
        "            model_2B, wlasl_train_loader, optimizer, criterion, device\n",
        "        )\n",
        "        val_loss, val_acc, val_f1, _, _ = eval_one_epoch_temporal(\n",
        "            model_2B, wlasl_val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        history_2B[\"train_loss\"].append(train_loss)\n",
        "        history_2B[\"train_acc\"].append(train_acc)\n",
        "        history_2B[\"train_f1\"].append(train_f1)\n",
        "        history_2B[\"val_loss\"].append(val_loss)\n",
        "        history_2B[\"val_acc\"].append(val_acc)\n",
        "        history_2B[\"val_f1\"].append(val_f1)\n",
        "\n",
        "        print(f\"[2B] Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train loss: {train_loss:.4f}, acc: {train_acc:.4f}, f1: {train_f1:.4f} | \"\n",
        "              f\"Val loss: {val_loss:.4f}, acc: {val_acc:.4f}, f1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state_2B = model_2B.state_dict().copy()\n",
        "            torch.save(best_state_2B, \"checkpoints/temporal_stage2B_best.pt\")\n",
        "\n",
        "    print(f\"\\nBest Stage 2B val macro-F1: {best_val_f1:.4f}\")\n",
        "    return history_2B, best_state_2B\n"
      ],
      "metadata": {
        "id": "Xf3VLn8uoc0S"
      },
      "id": "Xf3VLn8uoc0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_2A, state_2A = run_stage_2A(\n",
        "    num_epochs=5,\n",
        "    base_lr=1e-3,\n",
        "    num_wlasl_classes=num_wlasl_classes\n",
        ")\n",
        "\n",
        "history_2B, state_2B = run_stage_2B(\n",
        "    num_epochs=8,\n",
        "    base_lr=5e-4,\n",
        "    num_wlasl_classes=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "WJItp_u5YDJx"
      },
      "id": "WJItp_u5YDJx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_temporal_checkpoint(ckpt_path, num_wlasl_classes, loader, mode_name=\"Stage2\"):\n",
        "    # Build frame encoder with the same structure; we only need forward, so we can freeze_all=True\n",
        "    frame_encoder = ResNetFrameEncoder(\n",
        "        ckpt_path=BEST_CKPT,\n",
        "        num_classes=num_classes,\n",
        "        freeze_all=True,\n",
        "        unfreeze_layer4=False\n",
        "    )\n",
        "\n",
        "    model = ASLTemporalModel(\n",
        "        frame_encoder=frame_encoder,\n",
        "        feature_dim=512,\n",
        "        hidden_dim=256,\n",
        "        num_layers=1,\n",
        "        num_classes=num_wlasl_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Load temporal weights\n",
        "    state = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss, acc, f1, y_true, y_pred = eval_one_epoch_temporal(model, loader, criterion, device)\n",
        "\n",
        "    print(f\"[{mode_name}] Test loss: {loss:.4f}, acc: {acc:.4f}, macro-F1: {f1:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "    return loss, acc, f1, cm, y_true, y_pred\n"
      ],
      "metadata": {
        "id": "fmsOSxBfpD-u"
      },
      "id": "fmsOSxBfpD-u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick 2B because it clearly beats 2A\n",
        "BEST_STAGE2_CKPT = \"checkpoints/temporal_stage2B_best.pt\"\n",
        "\n",
        "test_loss, test_acc, test_f1, test_cm, y_true_test, y_pred_test = evaluate_temporal_checkpoint(\n",
        "    BEST_STAGE2_CKPT,\n",
        "    num_wlasl_classes=num_wlasl_classes,\n",
        "    loader=wlasl_test_loader,\n",
        "    mode_name=\"Stage 2B\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "uHWIEl04YDHx"
      },
      "id": "uHWIEl04YDHx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "daen429",
      "language": "python",
      "name": "daen429"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}